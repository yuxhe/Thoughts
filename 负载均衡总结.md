负载均衡总结（四层负载与七层负载的区别）
https://www.jianshu.com/p/9826d866080a
负载均衡总结（四层负载与七层负载的区别）
最近有几个应用要上线部署，在应用架构设计经常会应用到负载均衡。有很多同学搞不清楚七层负载与四层负载的区别，这里就做一个小小的总结。

什么是负载均衡
负载均衡 建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。

负载均衡分类
负载均衡根据所采用的设备对象（软/硬件负载均衡），应用的OSI网络层次（网络层次上的负载均衡），及应用的地理结构（本地/全局负载均衡）等来分类。本文着重介绍的是根据应用的 OSI 网络层次来分类的两个负载均衡类型。

我们先来看一张图，相信很多同学对这张图都不陌生，这是一张网络模型图，包含了 OSI 模型及 TCP/IP 模型，两个模型虽然有一点点区别，但主要的目的是一样的，模型图描述了通信是怎么进行的。它解决了实现有效通信所需要的所有过程，并将这些过程划分为逻辑上的层。层可以简单地理解成数据通信需要的步骤。

OSI_TCP/IP
根据负载均衡所作用在 OSI 模型的位置不同，负载均衡可以大概分为以下几类：

二层负载均衡（mac）

根据OSI模型分的二层负载，一般是用虚拟mac地址方式，外部对虚拟MAC地址请求，负载均衡接收后分配后端实际的MAC地址响应。

三层负载均衡（ip）

一般采用虚拟IP地址方式，外部对虚拟的ip地址请求，负载均衡接收后分配后端实际的IP地址响应。

四层负载均衡（tcp）

在三层负载均衡的基础上，用ip+port接收请求，再转发到对应的机器。

七层负载均衡（http）

根据虚拟的url或IP，主机名接收请求，再转向相应的处理服务器。

在实际应用中，比较常见的就是四层负载及七层负载。这里也重点说下这两种负载。

四层负载均衡（基于IP+端口的负载均衡）
所谓四层负载均衡，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。
layer4
layer4
在三层负载均衡的基础上，通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。

以常见的TCP为例，负载均衡设备在接收到第一个来自客户端的SYN 请求时，即通过上述方式选择一个最佳的服务器，并对报文中目标IP地址进行修改(改为后端服务器IP），直接转发给该服务器。TCP的连接建立，即三次握手是客户端和服务器直接建立的，负载均衡设备只是起到一个类似路由器的转发动作。在某些部署情况下，为保证服务器回包可以正确返回给负载均衡设备，在转发报文的同时可能还会对报文原来的源地址进行修改。

对应的负载均衡器称为四层交换机（L4 switch），主要分析IP层及TCP/UDP层，实现四层负载均衡。此种负载均衡器不理解应用协议（如HTTP/FTP/MySQL等等）
要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。

实现四层负载均衡的软件有：

F5：硬件负载均衡器，功能很好，但是成本很高。
lvs：重量级的四层负载软件
nginx：轻量级的四层负载软件，带缓存功能，正则表达式较灵活
haproxy：模拟四层转发，较灵活
4. 七层的负载均衡（基于虚拟的URL或主机IP的负载均衡)
所谓七层负载均衡，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。

layer7
layer7
在四层负载均衡的基础上（没有四层是绝对不可能有七层的），再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡。举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。

以常见的TCP为例，负载均衡设备如果要根据真正的应用层内容再选择服务器，只能先代理最终的服务器和客户端建立连接(三次握手)后，才可能接受到客户端发送的真正应用层内容的报文，然后再根据该报文中的特定字段，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。负载均衡设备在这种情况下，更类似于一个代理服务器。负载均衡和前端的客户端以及后端的服务器会分别建立TCP连接。所以从这个技术原理上来看，七层负载均衡明显的对负载均衡设备的要求更高，处理七层的能力也必然会低于四层模式的部署方式。

对应的负载均衡器称为七层交换机（L7 switch），除了支持四层负载均衡以外，还有分析应用层的信息，如HTTP协议URI或Cookie信息，实现七层负载均衡。此种负载均衡器能理解应用协议。

实现七层负载均衡的软件有：

haproxy：天生负载均衡技能，全面支持七层代理，会话保持，标记，路径转移；
nginx：只在http协议和mail协议上功能比较好，性能与haproxy差不多；
apache：功能较差
Mysql proxy：功能尚可。
5. 两者之间的区别
举个例子形象的说明：四层负载均衡就像银行的自助排号机，每一个达到银行的客户根据排号机的顺序，选择对应的窗口接受服务；而七层负载均衡像银行大堂经理，先确认客户需要办理的业务，再安排排号。这样办理理财、存取款等业务的客户，会根据银行内部资源得到统一协调处理，加快客户业务办理流程。

| | 四层负载均衡（layer 4） | 七层负载均衡（layer 7） |
+----------+-------------------------+--------------------------------------------------+
| 基于 | 基于IP+Port的 | 基于虚拟的URL或主机IP等。 |
+----------+-------------------------+--------------------------------------------------+
| 类似于 | 路由器 | 代理服务器 |
+----------+-------------------------+--------------------------------------------------+
| 握手次数 | 1 次 | 2 次 |
+----------+-------------------------+--------------------------------------------------+
| 复杂度 | 低 | 高 |
+----------+-------------------------+--------------------------------------------------+
| 性能 | 高；无需解析内容 | 中；需要算法识别 URL，Cookie 和 HTTP head 等信息 |
+----------+-------------------------


+--------------------------------------------------+
| 安全性 | 低，无法识别 DDoS等攻击 | 高， 可以防御SYN cookie以SYN flood等 |
+----------+-------------------------+--------------------------------------------------+
| 额外功能 | 无 | 会话保持，图片压缩，防盗链等 |
总结：从上面的对比看来四层负载与七层负载最大的区别就是效率与功能的区别。四层负载架构设计比较简单，无需解析具体的消息内容，在网络吞吐量及处理能力上会相对比较高，而七层负载均衡的优势则体现在功能多，控制灵活强大。在具体业务架构设计时，使用七层负载或者四层负载还得根据具体的情况综合考虑。


------------------------------------------
一、问题域
nginx、lvs、keepalived、f5、DNS轮询，每每提到这些技术，往往讨论的是接入层的这样几个问题：

1）可用性：任何一台机器挂了，服务受不受影响

2）扩展性：能否通过增加机器，扩充系统的性能

3）反向代理+负载均衡：请求是否均匀分摊到后端的操作单元执行

二、上面那些名词都是干嘛的
由于每个技术人的背景和知识域不同，上面那些名词缩写（运维的同学再熟悉不过了），还是花1分钟简单说明一下（详细请自行“百度”）：

1）nginx：一个高性能的web-server和实施反向代理的软件

2）lvs：Linux Virtual Server，使用集群技术，实现在linux操作系统层面的一个高性能、高可用、负载均衡服务器

3）keepalived：一款用来检测服务状态存活性的软件，常用来做高可用

4）f5：一个高性能、高可用、负载均衡的硬件设备（听上去和lvs功能差不多？）

5）DNS轮询：通过在DNS-server上对一个域名设置多个ip解析，来扩充web-server性能及实施负载均衡的技术

------------------------------------------
**1）通过DNS轮询来线性扩展入口lvs层的性能**

**2）通过keepalived来保证高可用**

**3）通过lvs来扩展多个nginx**

**4）通过nginx来做负载均衡，业务七层路由**


keepalived起初就是为lvs设计的   **心跳加重试机制**


lvs搭建笔记和高可用.txt
-------------------------------------------------------------------
LVS：

node01:
	ifconfig  eth0:8 192.168.150.100/24
node02~node03:
	1)修改内核：
		echo 1  >  /proc/sys/net/ipv4/conf/eth0/arp_ignore 
		echo 1  >  /proc/sys/net/ipv4/conf/all/arp_ignore 
		echo 2 > /proc/sys/net/ipv4/conf/eth0/arp_announce 
		echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce 
	2）设置隐藏的vip：
		ifconfig  lo:3  192.168.150.100  netmask 255.255.255.255
		
RS中的服务：
node02~node03:
	yum install httpd -y
	service httpd start
	vi   /var/www/html/index.html
		from 192.168.150.1x

LVS服务配置
node01:
		yum install ipvsadm 
	ipvsadm -A  -t  192.168.150.100:80  -s rr
	ipvsadm -a  -t 192.168.150.100:80  -r  192.168.150.12 -g -w 1
	ipvsadm -a  -t 192.168.150.100:80  -r  192.168.150.13 -g -w 1
	ipvsadm -ln

验证：
	浏览器访问  192.168.150.100   看到负载  疯狂F5
	node01：
		netstat -natp   结论看不到socket连接
	node02~node03:
		netstat -natp   结论看到很多的socket连接
	node01:
		ipvsadm -lnc    查看偷窥记录本
		TCP 00:57  FIN_WAIT    192.168.150.1:51587 192.168.150.100:80 192.168.150.12:80
		FIN_WAIT： 连接过，偷窥了所有的包
		SYN_RECV： 基本上lvs都记录了，证明lvs没事，一定是后边网络层出问题
	
	
	
	



keepalived实验：
主机： node01~node04

node01:
	ipvsadm -C
	ifconfig eth0:8 down

----------------------------
node01,node04:
	yum install keepalived ipvsadm -y
	配置：
		cd  /etc/keepalived/
		cp keepalived.conf keepalived.conf.bak
		vi keepalived.conf
			node01:
			vrrp：虚拟路由冗余协议！
				vrrp_instance VI_1 {
					state MASTER         //  node04  BACKUP
					interface eth0
					virtual_router_id 51
					priority 100		 //	 node04	 50
					advert_int 1
					authentication {
						auth_type PASS
						auth_pass 1111
					}
					virtual_ipaddress {
						192.168.150.100/24 dev eth0 label  eth0:3
					}
				}
			virtual_server 192.168.150.100 80 {
				delay_loop 6
				lb_algo rr
				lb_kind DR
				nat_mask 255.255.255.0
				persistence_timeout 0
				protocol TCP

				real_server 192.168.150.12 80 {
					weight 1
					HTTP_GET {
						url {
						  path /
						  status_code 200
						}
						connect_timeout 3
						nb_get_retry 3
						delay_before_retry 3
					}   
				}       
				real_server 192.168.150.13 80 {
					weight 1
					HTTP_GET {
						url {
						  path /
						  status_code 200
						}
						connect_timeout 3
						nb_get_retry 3
						delay_before_retry 3
					}
				}
			scp  ./keepalived.conf  root@node04:`pwd`

-----------------------------------------------------------

![](https://raw.githubusercontent.com/d3wolf/notes/918b66ea9ac4af6677130e9f1826abfd1080161b/images/tcp-1.png)




![](https://raw.githubusercontent.com/d3wolf/notes/918b66ea9ac4af6677130e9f1826abfd1080161b/images/tcp-2.jpg)


![](https://raw.githubusercontent.com/d3wolf/notes/918b66ea9ac4af6677130e9f1826abfd1080161b/images/tcp-4.jpg)
------------------------------------------------------------------

nginx高可用集群
操作系统：centos7

组件：lvs + keepalived

环境
vip: 192.168.8.25

master192.168.3.60

backup: 192.168.3.61

nginx1: 192.168.3.62

nginx2: 192.168.3.63

os: CentOS Linux release 7.3.1611 (Core)

kernal: Linux lvs2 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

graph TD
client[client] --> VIP{192.168.3.66}
  	VIP -->|vip| MASTER[master: 192.168.3.60]
    VIP -->|vip| BACKUP[backup: 192.168.3.61]
    MASTER --> |route| route[路由器]
    BACKUP --> |route| route[路由器]
    route -->|nginx1| nginx1[192.168.3.63] 
    route -->|nginx2| nginx2[192.168.3.64]
    
    style VIP fill:#ccf,stroke:#f66,stroke-width:2px,stroke-dasharray: 5, 5;
    
负载均衡模式：DR（直接路由）

更改镜像(所有节点)
$ mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak && curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
LVS节点
安装keepalive+lvs
#安装keepalived
$ yum install -y ipvsadm keepalived
$ systemctl enable keepalived
配置(backup节点修改配置：router_id，state，priority)
$ mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak
$ cat <<EOF > /etc/keepalived/keepalived.conf
! Configuration File for keepalived

global_defs {
   notification_email {
   	 #故障接受联系人
     notify@inspireso.org
   }
   #故障发送人
   notification_email_from notify@inspireso.org
   smtp_server 127.0.0.1
   smtp_connect_timeout 30
   #BACKUP上修改为LVS_BACKUP
   router_id LVS_MASTER
}

vrrp_instance VI_1 {
	#BACKUP上修改为BACKUP
    state MASTER
    interface eno16777984
    #虚拟路由标识，主从相同
    virtual_router_id 51
    #BACKUP上修改为90
    priority 100
    advert_int 1
    #主从认证密码必须一致
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    #虚拟IP（VTP）
    virtual_ipaddress {
        192.168.3.66
    }
}

#定义虚拟IP和端口
virtual_server 192.168.3.66 80 {
	#检查真实服务器时间，单位秒
    delay_loop 6
    #设置负载调度算法(rr|wrr|lc|wlc|lblc|dh|sh)
    ##rr:   轮叫调度(Round Robin，RR)
    ##wrr:  加权轮叫(Weighted Round Robin，WRR)
    ##dh:   目标地址散列(Destination Hashing,DH)
    ##sh:   源地址散列(Source Hashing,SH)
    
    ##lc:   最少链接(Least Connections，LC)
    ##wlc:  加权最少链接(Weighted Least Connections，WLC)
    ##lblc: 基于局部性的最少链接(Locality-Based Least Connections，LBLC)
    ##lblcr:带复制的基于局部性最少链接(Locality-Based Least Connections with Replication，LBLCR)
    ##sed:  最短的期望的延迟（Shortest Expected Delay Scheduling，SED）
    ##nq:   最少队列（Never Queue Scheduling，NQ）
    lb_algo sh
    
    #设置LVS负载均衡DR模式(DR|NAT|tun)
    lb_kind DR
    #同一IP的连接60秒内被分配到同一台真实服务器
    persistence_timeout 0
    #使用TCP协议检查realserver状态
    protocol TCP

	#nginx1
    real_server 192.168.3.62 80 {
    	#节点权重值
        weight 1
        #健康检查方式
        HTTP_GET {
            url {
              path /
              status_code 200
            }
            #连接超时
            connect_timeout 3
            #重试次数
            nb_get_retry 3
            #重试间隔/S
            delay_before_retry 3
        }
    }

	#nginx2
    real_server 192.168.3.63 80 {
        weight 1
        HTTP_GET {
            url {
              path /
              status_code 200
            }
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
        }
    }
}

EOF
启动服务
$ systemctl enable keepalived && systemctl restart keepalived
防火墙配置
#卸载firewalld
$ systemctl disable firewalld && yum remove -y firewalld
#安装iptables
$ yum install -y iptables-services

#配置防火墙参考
$ vi /etc/sysconfig/iptables
#默认禁止访问
:INPUT DROP [0:0]
:FORWARD DROP [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
# 允许vrrp协议
# master(192.168.3.60)添加如下规则
-A INPUT -s 192.168.3.61/24 -i eno16777984 -p vrrp -j ACCEPT
# backup(192.168.3.61)添加如下规则
-A INPUT -s 192.168.3.60/24 -i eno16777984 -p vrrp -j ACCEPT
#允许ssh
-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
#允许访问80，443
-A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 443 -j ACCEPT

-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT

#配置默认启动iptables并重启iptables防火墙
$ systemctl enable iptables.service && systemctl restart iptables.service
nginx节点
#开放80端口
$ firewall-cmd --zone=public --add-port=80/tcp --permanent
$ systemctl restart firewalld
$ yum install net-tools
$ echo <<EOF > /etc/init.d/real.sh
#!/bin/bash

VIP=192.168.3.66

case "$1" in
  start)
    ip addr add $VIP/32 broadcast $VIP dev lo label lo:0
    ip route add $VIP scope link src $VIP dev lo:0
    echo "1" >/proc/sys/net/ipv4/conf/lo/arp_ignore
    echo "2" >/proc/sys/net/ipv4/conf/lo/arp_announce
    echo "1" >/proc/sys/net/ipv4/conf/all/arp_ignore
    echo "2" >/proc/sys/net/ipv4/conf/all/arp_announce
    sysctl -p >/dev/null 2>&1
    echo "RealServer Start OK"
    ;;
  stop)
    ip addr del $VIP/32 dev lo:0
    #ip route del $VIP dev lo:0
    echo "0" >/proc/sys/net/ipv4/conf/lo/arp_ignore
    echo "0" >/proc/sys/net/ipv4/conf/lo/arp_announce
    echo "0" >/proc/sys/net/ipv4/conf/all/arp_ignore
    echo "0" >/proc/sys/net/ipv4/conf/all/arp_announce
    echo "RealServer Stoped"
    ;;
  *)
    echo "Usage: $0 {start|stop}"
    exit 1
esac

exit 0
EOF

$ chmod +x /etc/init.d/real.sh
$ /etc/init.d/real.sh start

# 配置开机自动启动
$ echo "/etc/init.d/real.sh start" >> /etc/rc.d/rc.local
$ chmod +x /etc/rc.d/rc.local
$ systemctl enable rc-local && systemctl start rc-local.service
防火墙配置
#卸载firewalld
$ systemctl disable firewalld && yum remove -y firewalld
#安装iptables
$ yum install -y iptables-services

#配置防火墙参考
$ vi /etc/sysconfig/iptables
#默认禁止访问
:INPUT DROP [0:0]
:FORWARD DROP [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
#允许ssh
-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT
#允许访问80，443
-A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT
-A INPUT -p tcp -m state --state NEW -m tcp --dport 443 -j ACCEPT

-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT

#配置默认启动iptables并重启iptables防火墙
$ systemctl enable iptables.service && systemctl restart iptables.service
ipvsadm
集群服务管理类命令
# 添加集群
ipvs -A -t|u|f service-address [-s scheduler]
-t: TCP协议的集群 
-u: UDP协议的集群 
    service-address:     IP:PORT 
-f: FWM: 防火墙标记 
    service-address: Mark Number
#示例
ipvsadm -A -t 172.16.1.253:80 -s wlc

# 修改集群
ipvs -E -t|u|f service-address [-s scheduler]
#示例
ipvsadm -E -t 172.16.1.253:80 -s wrr

#  删除集群
ipvsadm -D -t|u|f service-address
# 示例 
ipvsadm -D -t 172.16.1.253:80
管理集群中的RealServer
#  添加RS
ipvsadm -a -t|u|f service-address -r server-address [-g|i|m] [-w weight]
-t|u|f service-address：事先定义好的某集群服务 
-r server-address: 某RS的地址，在NAT模型中，可使用IP：PORT实现端口映射； 
[-g|i|m]: LVS类型    
    -g: DR 
    -i: TUN 
    -m: NAT 
[-w weight]: 定义服务器权重
#示例
ipvsadm -a -t 172.16.1.253:80 -r 172.16.1.101 –g -w 5
ipvsadm -a -t 172.16.1.253:80 -r 172.16.1.102 –g -w 10

# 修改RS
ipvsadm -e -t|u|f service-address -r server-address [-g|i|m] [-w weight]
#示例
ipvsadm -e -t 172.16.1.253:80 -r 172.16.1.101 –g -w 3

#  删除RS
ipvsadm -d -t|u|f service-address -r server-address
#示例
ipvsadm -d -t 172.16.1.253:80 -r 172.16.1.101
查看类
ipvsadm -L|l [options]
#常用选项[options]如下：
-n: 数字格式显示主机地址和端口 
--stats：统计数据 
--rate: 速率 
--timeout: 显示tcp、tcpfin和udp的会话超时时长 
-c: 显示当前的ipvs连接状况
其他管理类
#删除所有集群服务
ipvsadm -C
#保存规则
service ipvsadm save
ipvsadm -S > /path/to/somefile
其他注意事项
关于时间同步：各节点间的时间偏差不大于1s，建议使用统一的ntp服务器进行更新时间；
DR模型中的VIP的MAC广播问题：
在DR模型中，由于每个节点均要配置VIP，因此存在VIP的MAC广播问题，在现在的linux内核中，都提供了相应kernel 参数对MAC广播进行管理，具体如下：

arp_ignore: 定义接收到ARP请求时的响应级别；

0：只要本地配置的有相应地址，就给予响应； 1：仅在请求的目标地址配置在到达的接口上的时候，才给予响应；DR模型使用

arp_announce：定义将自己地址向外通告时的通告级别；

0：将本地任何接口上的任何地址向外通告； 1：试图仅向目标网络通告与其网络匹配的地址； 2：仅向与本地接口上地址匹配的网络进行通告；DR模型使用

---------------------
man有帮助文档  conf的帮助

**ifconfig  eth0  down 断网**   keepalived相互广播包

ps  -ef|wc -l 能统计个数

查看操作系统 最多可打开文件数
cat /proc/sys/fs/file-max    ； 查看所有的ulimit -a    每进程打开文件数 ulimit -n

184416

查看内存总量
cat /proc/meminfo |grep MemTotal
------------------------
1 查看CPU

1.1 查看CPU个数

# cat /proc/cpuinfo | grep "physical id" | uniq | wc -l

2 **uniq命令：删除重复行;wc –l命令：统计行数**

1.2 查看CPU核数

# cat /proc/cpuinfo | grep "cpu cores" | uniq
cpu cores : 4

1.3 查看CPU型号

# cat /proc/cpuinfo | grep 'model name' |uniq
model name : Intel(R) Xeon(R) CPU E5630 @ 2.53GHz

总结：该服务器有2个4核CPU，型号Intel(R) Xeon(R) CPU E5630 @ 2.53GHz

2 查看内存

2.1 查看内存总数

#cat /proc/meminfo | grep MemTotal
MemTotal: 32941268 kB //内存32G

 查看硬盘大小

# fdisk -l | grep Disk

查看硬盘使用情况
df  -h 

watch ifconfig 查看流量  

**nginx支持7层哈  nginx的location 代理配置中有/则 替换所有的，否则无则把其连接到其后面**

man 5 *.conf        /aaa 回车   b向上翻页

keepalived+nginx高可用架


**rpm -ql 包名 列出一个rpm包安装的文件**


**happroxy 也是支持7层的负载均衡代理服务器**   （类似lvs 在linux内核中）

yum install man-pages

man 2 socket 帮助手册 系统类

https://www.bilibili.com/video/BV1oZ4y1T7N2?p=2






